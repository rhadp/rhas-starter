kind: Task
apiVersion: tekton.dev/v1
metadata:
  name: s3-upload
spec:
  params:
    # file parameters
    - name: SOURCE
      description: "The source file"
      type: string
    - name: DESTINATION
      description: "The destination file"
      type: string
    - name: BUCKET
      description: "The S3 bucket name"
      type: string
      default: "rhadp-aib-cdn"
    # AWS credentials
    - name: AWS_SECRETS_REF
      description: "Secret with the access key and secret key"
      type: string
      default: "aws-credentials"
    # scripts configuration
    - name: SCRIPT
      description: "Script to be executed"
      type: string
      default: "/home/user/s3-upload.py"

  steps:
    - name: s3-upload
      image: ghcr.io/rhadp/pipeline:latest
      env:
        - name: AWS_ACCESS_KEY_ID
          valueFrom:
            secretKeyRef:
              key: aws_access_key_id
              name: $(params.AWS_SECRETS_REF)
        - name: AWS_SECRET_ACCESS_KEY
          valueFrom:
            secretKeyRef:
              key: aws_secret_access_key
              name: $(params.AWS_SECRETS_REF)
        - name: AWS_REGION
          valueFrom:
            secretKeyRef:
              key: aws_default_region
              name: $(params.AWS_SECRETS_REF)

      script: |
        #!/usr/bin/env bash

        set -euo pipefail
        source /venv/bin/activate

        # run the script
        python "$(params.SCRIPT)" $(params.SOURCE) -b $(params.BUCKET) -d $(params.DESTINATION)

  workspaces:
    - name: source